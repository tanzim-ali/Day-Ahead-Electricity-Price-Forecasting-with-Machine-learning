{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1VsErfyohOjnjmeXm9lKJH73EdCxp6lJ_","authorship_tag":"ABX9TyOPod1YxIuFboZgr7533FuI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6NkjXJO5ndRf","executionInfo":{"status":"ok","timestamp":1710952673669,"user_tz":-480,"elapsed":2711,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import numpy as np"]},{"cell_type":"code","source":["# Load the datasets\n","train_df = pd.read_csv('/content/drive/MyDrive/FYP_2024/Final_Train.csv')\n","test_df = pd.read_csv('/content/drive/MyDrive/FYP_2024/Final_test.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/FYP_2024/Final_Validation.csv')"],"metadata":{"id":"hmrU0NUsoLKP","executionInfo":{"status":"ok","timestamp":1710952733853,"user_tz":-480,"elapsed":1101,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to preprocess datasets\n","def preprocess_data(df):\n","    df['local_time'] = pd.to_datetime(df['local_time'])\n","    for time_unit in ['Year', 'Month', 'Day', 'Hour']:\n","        df[time_unit] = getattr(df['local_time'].dt, time_unit.lower())\n","    return df.drop('local_time', axis=1)"],"metadata":{"id":"FWwgK8XJoOBS","executionInfo":{"status":"ok","timestamp":1710952738276,"user_tz":-480,"elapsed":375,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def add_cyclical_features(df, col_name, max_val):\n","    if col_name in df.columns:\n","        df[col_name + '_sin'] = np.sin(2 * np.pi * df[col_name] / max_val)\n","        df[col_name + '_cos'] = np.cos(2 * np.pi * df[col_name] / max_val)\n","    return df\n","\n","#'Hour', 'Day', and 'Month' created\n","for df in [train_df, validation_df, test_df]:\n","    df = add_cyclical_features(df, 'Hour', 23)\n","    df = add_cyclical_features(df, 'Day', 31)\n","    df = add_cyclical_features(df, 'Month', 12)"],"metadata":{"id":"4PGjV3TVoQg-","executionInfo":{"status":"ok","timestamp":1710952741158,"user_tz":-480,"elapsed":365,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def add_lagged_features(df, feature_cols, n_lags=3):\n","    for feature in feature_cols:\n","        for lag in range(1, n_lags + 1):\n","            df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n","    return df.dropna().reset_index(drop=True)\n","\n","# Preprocess all datasets\n","train_df = preprocess_data(train_df)\n","test_df = preprocess_data(test_df)\n","validation_df = preprocess_data(validation_df)\n","\n","# Specifying the features to lag\n","features_to_lag = ['Average_Temp', 'MW']  # Add other relevant features as needed\n","\n","# Applying the function to the datasets\n","train_df = add_lagged_features(train_df, features_to_lag, n_lags=3)\n","validation_df = add_lagged_features(validation_df, features_to_lag, n_lags=3)\n","test_df = add_lagged_features(test_df, features_to_lag, n_lags=3)"],"metadata":{"id":"uPTP8kq2oTMG","executionInfo":{"status":"ok","timestamp":1710952743164,"user_tz":-480,"elapsed":2,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","# Creating polynomial and interaction features\n","poly = PolynomialFeatures(degree=2, include_bias=False)\n","\n","# Applying transformation to all datasets.\n","train_features = train_df.drop(columns=['MW'])\n","validation_features = validation_df.drop(columns=['MW'])\n","test_features = test_df.drop(columns=['MW'])\n","\n","X_train_poly = poly.fit_transform(train_features)\n","X_validation_poly = poly.transform(validation_features)\n","X_test_poly = poly.transform(test_features)\n","\n","\n","feature_names = poly.get_feature_names_out(input_features=train_features.columns)\n","\n","\n","y_train = train_df['MW']\n","y_validation = validation_df['MW']\n","y_test = test_df['MW']"],"metadata":{"id":"D1_3xTI5oV45","executionInfo":{"status":"ok","timestamp":1710952746185,"user_tz":-480,"elapsed":2,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Evaluation Function\n","\n","def calculate_metrics(actual, predicted, lower_bound=0, upper_bound=100, iqr_multiplier=1.5):\n","    # Excluding negative actual values if considered invalid\n","    valid_indices = actual > lower_bound\n","    actual = actual[valid_indices]\n","    predicted = predicted[valid_indices]\n","\n","    # Calculate MAE and RMSE\n","    mae = mean_absolute_error(actual, predicted)\n","    rmse = np.sqrt(mean_squared_error(actual, predicted))\n","\n","    # Thresholding for outlier exclusion based on IQR\n","    q1, q3 = np.percentile(actual, [25, 75])\n","    iqr = q3 - q1\n","    outlier_threshold_upper = q3 + (iqr * iqr_multiplier)\n","    outlier_threshold_lower = q1 - (iqr * iqr_multiplier)\n","\n","    valid_indices_for_mape = (actual >= outlier_threshold_lower) & (actual <= outlier_threshold_upper)\n","    filtered_actual = actual[valid_indices_for_mape]\n","    filtered_predicted = predicted[valid_indices_for_mape]\n","\n","    # Calculate Modified MAPE with capped at 100%\n","    if len(filtered_actual) > 0:\n","        percentage_errors = np.abs((filtered_predicted - filtered_actual) / filtered_actual) * 100\n","        percentage_errors = np.clip(percentage_errors, None, upper_bound)  # Cap percentage errors at upper_bound (100%)\n","        mape = np.mean(percentage_errors)\n","    else:\n","        mape = np.nan\n","\n","    # Calculate sMAPE\n","    smape = 100/len(actual) * np.sum(2 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted)))\n","\n","    return mae, mape, smape, rmse"],"metadata":{"id":"3NjJi0HNobOI","executionInfo":{"status":"ok","timestamp":1710952749323,"user_tz":-480,"elapsed":373,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n","from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from keras.regularizers import l2\n","import numpy as np\n","\n","def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n","    '''\n","    Wrapper function to create a LearningRateScheduler with step decay schedule.\n","    '''\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch / step_size))\n","    return LearningRateScheduler(schedule)\n","\n","# Ensure X_train_poly, X_validation_poly, and X_test_poly are numpy arrays for LSTM\n","X_train_lstm = np.array(X_train_poly)\n","X_validation_lstm = np.array(X_validation_poly)\n","X_test_lstm = np.array(X_test_poly)\n","\n","# Reshape the data for LSTM [samples, time steps, features]\n","# Assuming that we are treating each sample as an independent observation with 1 time step\n","X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], 1, X_train_lstm.shape[1]))\n","X_validation_lstm = X_validation_lstm.reshape((X_validation_lstm.shape[0], 1, X_validation_lstm.shape[1]))\n","X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], 1, X_test_lstm.shape[1]))\n","\n","# Confirming the shape\n","print(\"X_train shape:\", X_train_lstm.shape)\n","print(\"X_validation shape:\", X_validation_lstm.shape)\n","print(\"X_test shape:\", X_test_lstm.shape)\n","\n","# Model definition\n","model = Sequential([\n","    LSTM(100, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), kernel_regularizer=l2(0.01)),\n","    Dropout(0.3),\n","    BatchNormalization(),\n","    Dense(1, kernel_regularizer=l2(0.01))\n","])\n","\n","model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n","\n","# Callbacks\n","checkpoint_path = '/content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5'\n","checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n","lr_scheduler = step_decay_schedule(initial_lr=0.001, decay_factor=0.75, step_size=10)\n","\n","# Model training\n","history = model.fit(\n","    X_train_lstm, y_train,\n","    epochs=50,\n","    batch_size=64,\n","    validation_data=(X_validation_lstm, y_validation),\n","    callbacks=[checkpoint, early_stopping, lr_scheduler],\n","    verbose=1\n",")\n","\n","# Load the best model after training\n","model.load_weights(checkpoint_path)\n","print(\"Model training complete. Best model loaded.\")\n","\n","model.save(checkpoint_path)\n","print(\"Model saved at: {}\".format(checkpoint_path))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6GVSOrWnowYo","outputId":"58eb8cae-b085-45dd-bde9-35dd2ebf76ee","executionInfo":{"status":"ok","timestamp":1710952806645,"user_tz":-480,"elapsed":47872,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (17516, 1, 77)\n","X_validation shape: (8758, 1, 77)\n","X_test shape: (1437, 1, 77)\n","Epoch 1/50\n","271/274 [============================>.] - ETA: 0s - loss: 8460.3750\n","Epoch 1: val_loss improved from inf to 3989.94751, saving model to /content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r274/274 [==============================] - 8s 18ms/step - loss: 8433.1094 - val_loss: 3989.9475 - lr: 0.0010\n","Epoch 2/50\n","269/274 [============================>.] - ETA: 0s - loss: 5843.9248\n","Epoch 2: val_loss improved from 3989.94751 to 2107.23267, saving model to /content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5\n","274/274 [==============================] - 2s 9ms/step - loss: 5855.5244 - val_loss: 2107.2327 - lr: 0.0010\n","Epoch 3/50\n","269/274 [============================>.] - ETA: 0s - loss: 4229.1870\n","Epoch 3: val_loss improved from 2107.23267 to 1915.45935, saving model to /content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5\n","274/274 [==============================] - 2s 8ms/step - loss: 4213.5562 - val_loss: 1915.4594 - lr: 0.0010\n","Epoch 4/50\n","272/274 [============================>.] - ETA: 0s - loss: 3985.0708\n","Epoch 4: val_loss did not improve from 1915.45935\n","274/274 [==============================] - 2s 6ms/step - loss: 3974.2019 - val_loss: 1977.6581 - lr: 0.0010\n","Epoch 5/50\n","271/274 [============================>.] - ETA: 0s - loss: 3907.0903\n","Epoch 5: val_loss did not improve from 1915.45935\n","274/274 [==============================] - 2s 7ms/step - loss: 3929.9512 - val_loss: 1982.4744 - lr: 0.0010\n","Epoch 6/50\n","269/274 [============================>.] - ETA: 0s - loss: 3784.7017\n","Epoch 6: val_loss improved from 1915.45935 to 1911.01672, saving model to /content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5\n","274/274 [==============================] - 2s 8ms/step - loss: 3856.0347 - val_loss: 1911.0167 - lr: 0.0010\n","Epoch 7/50\n","271/274 [============================>.] - ETA: 0s - loss: 4066.1189\n","Epoch 7: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 3s 11ms/step - loss: 4048.3589 - val_loss: 2011.4409 - lr: 0.0010\n","Epoch 8/50\n","264/274 [===========================>..] - ETA: 0s - loss: 4108.9370\n","Epoch 8: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 7ms/step - loss: 4066.0220 - val_loss: 1973.8843 - lr: 0.0010\n","Epoch 9/50\n","271/274 [============================>.] - ETA: 0s - loss: 4038.7085\n","Epoch 9: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 9ms/step - loss: 4030.9458 - val_loss: 2061.4646 - lr: 0.0010\n","Epoch 10/50\n","270/274 [============================>.] - ETA: 0s - loss: 4129.0425\n","Epoch 10: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 3s 12ms/step - loss: 4102.1855 - val_loss: 2254.2771 - lr: 0.0010\n","Epoch 11/50\n","269/274 [============================>.] - ETA: 0s - loss: 4127.9590\n","Epoch 11: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 8ms/step - loss: 4116.2681 - val_loss: 2009.5339 - lr: 7.5000e-04\n","Epoch 12/50\n","270/274 [============================>.] - ETA: 0s - loss: 4098.7944\n","Epoch 12: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 3s 11ms/step - loss: 4114.3096 - val_loss: 2044.5398 - lr: 7.5000e-04\n","Epoch 13/50\n","267/274 [============================>.] - ETA: 0s - loss: 4067.2908\n","Epoch 13: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 8ms/step - loss: 4114.9565 - val_loss: 2021.5529 - lr: 7.5000e-04\n","Epoch 14/50\n","271/274 [============================>.] - ETA: 0s - loss: 4119.0967\n","Epoch 14: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 7ms/step - loss: 4111.7241 - val_loss: 2052.4790 - lr: 7.5000e-04\n","Epoch 15/50\n","274/274 [==============================] - ETA: 0s - loss: 4104.6226\n","Epoch 15: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 6ms/step - loss: 4104.6226 - val_loss: 2002.6050 - lr: 7.5000e-04\n","Epoch 16/50\n","274/274 [==============================] - ETA: 0s - loss: 4106.1133\n","Epoch 16: val_loss did not improve from 1911.01672\n","274/274 [==============================] - 2s 7ms/step - loss: 4106.1133 - val_loss: 2013.0808 - lr: 7.5000e-04\n","Epoch 16: early stopping\n","Model training complete. Best model loaded.\n","Model saved at: /content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5\n"]}]},{"cell_type":"code","source":["from keras.models import load_model\n","\n","# Load the trained model\n","model = load_model('/content/drive/MyDrive/Saved_trained_models/best_rnn_model_V3.h5')"],"metadata":{"id":"cFqCc8oYe8Ys","executionInfo":{"status":"ok","timestamp":1710952838400,"user_tz":-480,"elapsed":2134,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Ensure X_validation_poly is a numpy array\n","X_validation_poly_array = np.array(X_validation_poly)\n","\n","# Reshape X_validation_poly for LSTM\n","# Adding the time step dimension\n","X_validation_poly_reshaped = X_validation_poly_array.reshape((X_validation_poly_array.shape[0], 1, X_validation_poly_array.shape[1]))\n","\n","# Now you can predict using the reshaped validation data\n","predictions = model.predict(X_validation_poly_reshaped)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ_8vOhPvObY","executionInfo":{"status":"ok","timestamp":1710953004292,"user_tz":-480,"elapsed":2615,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}},"outputId":"27f8ee5e-bcbf-4197-c834-73ac896f9e02"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["274/274 [==============================] - 2s 5ms/step\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Ensure X_train_poly is a numpy array\n","X_train_poly_array = np.array(X_train_poly)\n","\n","# Reshape X_train_poly for LSTM\n","# Adding the time step dimension\n","X_train_poly_reshaped = X_train_poly_array.reshape((X_train_poly_array.shape[0], 1, X_train_poly_array.shape[1]))\n","\n","# Predict using the reshaped training data\n","train_predictions = model.predict(X_train_poly_reshaped)\n","\n","# Flatten predictions to match the shape of y_train for evaluation\n","train_predictions_flattened = train_predictions.flatten()\n","\n","# Evaluate the model on the training set using your custom function\n","train_mae, train_mape, train_smape, train_rmse = calculate_metrics(y_train, train_predictions_flattened)\n","\n","# Print the evaluation metrics for the training set\n","print(f\"Training Set - MAE: {train_mae}, MAPE: {train_mape}, sMAPE: {train_smape}, RMSE: {train_rmse}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9RtVIEwvz8g","executionInfo":{"status":"ok","timestamp":1710953121776,"user_tz":-480,"elapsed":5823,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}},"outputId":"4997cedf-b236-42ee-a406-902d79b4b2c9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["548/548 [==============================] - 3s 5ms/step\n","Training Set - MAE: 31.733617338091808, MAPE: 42.23611141566295, sMAPE: 42.14225898776994, RMSE: 61.038844805006136\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Assuming you've already made predictions and have your model and data ready\n","\n","# Ensure predictions are 1D\n","predictions = predictions.squeeze()\n","\n","# If necessary, also ensure y_validation is 1D\n","# This line may be unnecessary if y_validation is already 1D\n","y_validation_array = np.array(y_validation)  # Ensure y_validation is a numpy array\n","y_validation_squeezed = y_validation_array.squeeze()\n","\n","# Now, evaluate the model using your custom function\n","mae, mape, smape, rmse = calculate_metrics(y_validation_squeezed, predictions)\n","\n","# Print the evaluation metrics\n","print(f\"MAE: {mae}, MAPE: {mape}, sMAPE: {smape}, RMSE: {rmse}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUrwa7O2weJb","executionInfo":{"status":"ok","timestamp":1710953214827,"user_tz":-480,"elapsed":5,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}},"outputId":"34c8b5f2-5c51-4577-a191-3845036b4467"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE: 29.814441659553065, MAPE: 47.41280686745494, sMAPE: 47.71087478952085, RMSE: 42.93988185280797\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Assuming your model is already trained and you have the test features and labels ready\n","\n","# Ensure X_test_poly is a numpy array\n","X_test_poly_array = np.array(X_test_poly)\n","\n","# Reshape X_test_poly for LSTM [samples, time steps, features]\n","X_test_poly_reshaped = X_test_poly_array.reshape((X_test_poly_array.shape[0], 1, X_test_poly_array.shape[1]))\n","\n","# Predict using the reshaped test data\n","test_predictions = model.predict(X_test_poly_reshaped)\n","\n","# Ensure predictions are 1D\n","test_predictions_squeezed = test_predictions.squeeze()\n","\n","# If necessary, also ensure y_test is 1D\n","# This line may be unnecessary if y_test is already 1D\n","y_test_array = np.array(y_test)  # Ensure y_test is a numpy array\n","y_test_squeezed = y_test_array.squeeze()\n","\n","# Now, evaluate the model using your custom function\n","test_mae, test_mape, test_smape, test_rmse = calculate_metrics(y_test_squeezed, test_predictions_squeezed)\n","\n","# Print the evaluation metrics for the test set\n","print(f\"Test Set - MAE: {test_mae}, MAPE: {test_mape}, sMAPE: {test_smape}, RMSE: {test_rmse}\")\n"],"metadata":{"id":"eQmWH6dNx4q1","executionInfo":{"status":"ok","timestamp":1710953570798,"user_tz":-480,"elapsed":2935,"user":{"displayName":"TAHSEEN Ali","userId":"03400057957201749607"}},"outputId":"ea4e9448-2c61-4bc5-b262-63d5ed07311a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["45/45 [==============================] - 1s 16ms/step\n","Test Set - MAE: 28.294001512065865, MAPE: 47.200871079531794, sMAPE: 42.48477458613127, RMSE: 38.941466614452814\n"]}]}]}