{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('/Users/alitahseen/Desktop/FYP-2024/Machine_learning/Datafiles/Final_Train.csv')\n",
    "test_df = pd.read_csv('/Users/alitahseen/Desktop/FYP-2024/Machine_learning/Datafiles/Final_test.csv')\n",
    "validation_df = pd.read_csv('/Users/alitahseen/Desktop/FYP-2024/Machine_learning/Datafiles/Final_Validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess datasets\n",
    "def preprocess_data(df):\n",
    "    df['local_time'] = pd.to_datetime(df['local_time'])\n",
    "    for time_unit in ['Year', 'Month', 'Day', 'Hour']:\n",
    "        df[time_unit] = getattr(df['local_time'].dt, time_unit.lower())\n",
    "    return df.drop('local_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all datasets\n",
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "validation_df = preprocess_data(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the feature sets and target variables\n",
    "def get_features_targets(df):\n",
    "    X = df[['Year', 'Month', 'Day', 'Hour', 'Average_Temp']]\n",
    "    y = df['MW']\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = get_features_targets(train_df)\n",
    "X_validation, y_validation = get_features_targets(validation_df)\n",
    "X_test, y_test = get_features_targets(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def calculate_metrics(actual, predicted, lower_bound=0, upper_bound=100, iqr_multiplier=1.5):\n",
    "    # Excluding negative actual values if considered invalid\n",
    "    valid_indices = actual > lower_bound\n",
    "    actual = actual[valid_indices]\n",
    "    predicted = predicted[valid_indices]\n",
    "\n",
    "    # Calculate MAE and RMSE\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "    # Thresholding for outlier exclusion based on IQR\n",
    "    q1, q3 = np.percentile(actual, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    outlier_threshold_upper = q3 + (iqr * iqr_multiplier)\n",
    "    outlier_threshold_lower = q1 - (iqr * iqr_multiplier)\n",
    "\n",
    "    valid_indices_for_mape = (actual >= outlier_threshold_lower) & (actual <= outlier_threshold_upper)\n",
    "    filtered_actual = actual[valid_indices_for_mape]\n",
    "    filtered_predicted = predicted[valid_indices_for_mape]\n",
    "\n",
    "    # Calculate Modified MAPE with capped at 100%\n",
    "    if len(filtered_actual) > 0:\n",
    "        percentage_errors = np.abs((filtered_predicted - filtered_actual) / filtered_actual) * 100\n",
    "        percentage_errors = np.clip(percentage_errors, None, upper_bound)  # Cap percentage errors at upper_bound (100%)\n",
    "        mape = np.mean(percentage_errors)\n",
    "    else:\n",
    "        mape = np.nan\n",
    "\n",
    "    # Calculate sMAPE\n",
    "    smape = 100/len(actual) * np.sum(2 * np.abs(predicted - actual) / (np.abs(actual) + np.abs(predicted)))\n",
    "\n",
    "    return mae, mape, smape, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'alpha': 1}\n",
      "Best model score: -33.475504118378204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/alitahseen/Desktop/FYP-2024/Machine_learning/notebooks/Trained_Models/best_lasso_model.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from joblib import dump\n",
    "\n",
    "# Defining the parameter grid to search over\n",
    "param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# grid_search = GridSearchCV(Lasso(), param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search = GridSearchCV(Lasso(), param_grid, cv=10, scoring='neg_mean_absolute_error', verbose=1)\n",
    "\n",
    "\n",
    "# Fitting the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best Lasso model\n",
    "best_lasso_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best model parameters:\", grid_search.best_params_)\n",
    "print(\"Best model score:\", grid_search.best_score_)\n",
    "# Save the model to a file\n",
    "model_filename = '/Users/alitahseen/Desktop/FYP-2024/Machine_learning/notebooks/Trained_Models/best_lasso_model.joblib'\n",
    "dump(best_lasso_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "model_filename = '/Users/alitahseen/Desktop/FYP-2024/Machine_learning/notebooks/Trained_Models/best_lasso_model.joblib'\n",
    "\n",
    "best_lasso_model = load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: MAE=71.17797745704087, MAPE=83.34047995815398, sMAPE=81.74729869145007, RMSE=79.54054735399076\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the best model on the validation dataset\n",
    "y_validation_pred = best_lasso_model.predict(X_validation)\n",
    "validation_metrics = calculate_metrics(y_validation, y_validation_pred)\n",
    "print(f\"Validation Metrics: MAE={validation_metrics[0]}, MAPE={validation_metrics[1]}, sMAPE={validation_metrics[2]}, RMSE={validation_metrics[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: MAE=69.98444526868717, MAPE=92.61714476822613, sMAPE=81.04086521533716, RMSE=72.81747677952124\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the best model on the test dataset\n",
    "y_test_pred = best_lasso_model.predict(X_test)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "print(f\"Test Metrics: MAE={test_metrics[0]}, MAPE={test_metrics[1]}, sMAPE={test_metrics[2]}, RMSE={test_metrics[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
