{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAR Training Dataset Head:\n",
      "     Average_Temp  hour  day_of_week        MW  MW_lag_1  MW_lag_2  MW_lag_7\n",
      "7          3.684     7            4  38.24962  40.34799  36.87143  34.02723\n",
      "8          3.834     8            4  29.23187  38.24962  40.34799  32.25537\n",
      "9          4.966     9            4  22.11242  29.23187  38.24962  33.48902\n",
      "10         6.308    10            4  16.62852  22.11242  29.23187  32.79130\n",
      "11         7.726    11            4  14.51898  16.62852  22.11242  34.10638\n",
      "DNN Training Dataset Head:\n",
      "    Average_Temp      hour  day_of_week        MW\n",
      "0     -1.213493 -1.660760     0.496702 -0.613609\n",
      "1     -1.300242 -1.516297     0.496702 -0.641057\n",
      "2     -1.347697 -1.371834     0.496702 -0.621946\n",
      "3     -1.318378 -1.227370     0.496702 -0.632755\n",
      "4     -1.382760 -1.082907     0.496702 -0.612383\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/alitahseen/Desktop/FYP-2024/Machine_learning/Datafiles/ML_Dataset.csv'\n",
    "original_dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Datetime' to datetime and sort by this column\n",
    "original_dataset['Datetime'] = pd.to_datetime(original_dataset['Datetime'])\n",
    "original_dataset.sort_values('Datetime', inplace=True)\n",
    "\n",
    "# Extracting time-related features from 'Datetime'\n",
    "original_dataset['hour'] = original_dataset['Datetime'].dt.hour\n",
    "original_dataset['day_of_week'] = original_dataset['Datetime'].dt.dayofweek\n",
    "\n",
    "# Make two copies of the dataset\n",
    "df1 = original_dataset.copy()  # For LEAR\n",
    "df2 = original_dataset.copy()  # For DNN\n",
    "\n",
    "# --- Preparing df1 for LEAR with lagged features ---\n",
    "def create_lagged_features(df, target, lag_days):\n",
    "    for lag in lag_days:\n",
    "        df[f'{target}_lag_{lag}'] = df[target].shift(lag)\n",
    "    return df.dropna()\n",
    "\n",
    "# Creating lagged features for LEAR\n",
    "lag_days = [1, 2, 7]\n",
    "df1 = create_lagged_features(df1, 'MW', lag_days)\n",
    "\n",
    "# Calculate indices for splitting (70% train, 15% validate, 15% test)\n",
    "split_idx_lear_train = int(len(df1) * 0.7)\n",
    "split_idx_lear_val = split_idx_lear_train + int(len(df1) * 0.15)\n",
    "\n",
    "# Splitting df1 for LEAR\n",
    "feature_columns_lear = ['Average_Temp', 'hour', 'day_of_week', 'MW'] + [f'MW_lag_{lag}' for lag in lag_days]\n",
    "X_train_lear = df1.iloc[:split_idx_lear_train][feature_columns_lear]\n",
    "y_train_lear = df1.iloc[:split_idx_lear_train]['MW']\n",
    "X_val_lear = df1.iloc[split_idx_lear_train:split_idx_lear_val][feature_columns_lear]\n",
    "y_val_lear = df1.iloc[split_idx_lear_train:split_idx_lear_val]['MW']\n",
    "X_test_lear = df1.iloc[split_idx_lear_val:][feature_columns_lear]\n",
    "y_test_lear = df1.iloc[split_idx_lear_val:]['MW']\n",
    "\n",
    "# --- Preparing df2 for DNN with normalized features ---\n",
    "feature_columns_dnn = ['Average_Temp', 'hour', 'day_of_week', 'MW']\n",
    "\n",
    "# Calculate indices for splitting (similarly 70-15-15 split)\n",
    "split_idx_dnn_train = int(len(df2) * 0.7)\n",
    "split_idx_dnn_val = split_idx_dnn_train + int(len(df2) * 0.15)\n",
    "\n",
    "# Splitting df2 for DNN\n",
    "X_train_dnn = df2.iloc[:split_idx_dnn_train][feature_columns_dnn]\n",
    "y_train_dnn = df2.iloc[:split_idx_dnn_train]['MW']\n",
    "X_val_dnn = df2.iloc[split_idx_dnn_train:split_idx_dnn_val][feature_columns_dnn]\n",
    "y_val_dnn = df2.iloc[split_idx_dnn_train:split_idx_dnn_val]['MW']\n",
    "X_test_dnn = df2.iloc[split_idx_dnn_val:][feature_columns_dnn]\n",
    "y_test_dnn = df2.iloc[split_idx_dnn_val:]['MW']\n",
    "\n",
    "# Normalizing features for DNN\n",
    "scaler = StandardScaler()\n",
    "X_train_dnn_scaled = scaler.fit_transform(X_train_dnn)\n",
    "X_val_dnn_scaled = scaler.transform(X_val_dnn)\n",
    "X_test_dnn_scaled = scaler.transform(X_test_dnn)\n",
    "\n",
    "# Print the first few rows of the training datasets for verification\n",
    "print(\"LEAR Training Dataset Head:\\n\", X_train_lear.head())\n",
    "print(\"DNN Training Dataset Head:\\n\", pd.DataFrame(X_train_dnn_scaled, columns=X_train_dnn.columns).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       local_time_orig  Average_Temp_orig PNODE_RESMRID_orig GRP_TYPE_orig  \\\n",
      "0  2021-01-01 00:00:00              5.186   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "1  2021-01-01 01:00:00              4.612   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "2  2021-01-01 02:00:00              4.298   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "3  2021-01-01 03:00:00              4.492   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "4  2021-01-01 04:00:00              4.066   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "5  2021-01-01 05:00:00              4.208   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "6  2021-01-01 06:00:00              3.968   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "7  2021-01-01 07:00:00              3.684   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "8  2021-01-01 08:00:00              3.834   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "9  2021-01-01 09:00:00              4.966   TH_NP15_GEN-APND   ALL_APNODES   \n",
      "\n",
      "   POS_orig   MW_orig  GROUP_orig            Datetime  hour_orig  \\\n",
      "0         0  34.02723           1 2021-01-01 00:00:00          0   \n",
      "1         0  32.25537           1 2021-01-01 01:00:00          1   \n",
      "2         0  33.48902           1 2021-01-01 02:00:00          2   \n",
      "3         0  32.79130           1 2021-01-01 03:00:00          3   \n",
      "4         0  34.10638           1 2021-01-01 04:00:00          4   \n",
      "5         0  36.87143           1 2021-01-01 05:00:00          5   \n",
      "6         0  40.34799           1 2021-01-01 06:00:00          6   \n",
      "7         0  38.24962           1 2021-01-01 07:00:00          7   \n",
      "8         0  29.23187           1 2021-01-01 08:00:00          8   \n",
      "9         0  22.11242           1 2021-01-01 09:00:00          9   \n",
      "\n",
      "   day_of_week_orig  ... PNODE_RESMRID_transformed  GRP_TYPE_transformed  \\\n",
      "0                 4  ...                       NaN                   NaN   \n",
      "1                 4  ...                       NaN                   NaN   \n",
      "2                 4  ...                       NaN                   NaN   \n",
      "3                 4  ...                       NaN                   NaN   \n",
      "4                 4  ...                       NaN                   NaN   \n",
      "5                 4  ...                       NaN                   NaN   \n",
      "6                 4  ...                       NaN                   NaN   \n",
      "7                 4  ...          TH_NP15_GEN-APND           ALL_APNODES   \n",
      "8                 4  ...          TH_NP15_GEN-APND           ALL_APNODES   \n",
      "9                 4  ...          TH_NP15_GEN-APND           ALL_APNODES   \n",
      "\n",
      "  POS_transformed MW_transformed  GROUP_transformed  hour_transformed  \\\n",
      "0             NaN            NaN                NaN               NaN   \n",
      "1             NaN            NaN                NaN               NaN   \n",
      "2             NaN            NaN                NaN               NaN   \n",
      "3             NaN            NaN                NaN               NaN   \n",
      "4             NaN            NaN                NaN               NaN   \n",
      "5             NaN            NaN                NaN               NaN   \n",
      "6             NaN            NaN                NaN               NaN   \n",
      "7             0.0       38.24962                1.0               7.0   \n",
      "8             0.0       29.23187                1.0               8.0   \n",
      "9             0.0       22.11242                1.0               9.0   \n",
      "\n",
      "   day_of_week_transformed  MW_lag_1  MW_lag_2  MW_lag_7  \n",
      "0                      NaN       NaN       NaN       NaN  \n",
      "1                      NaN       NaN       NaN       NaN  \n",
      "2                      NaN       NaN       NaN       NaN  \n",
      "3                      NaN       NaN       NaN       NaN  \n",
      "4                      NaN       NaN       NaN       NaN  \n",
      "5                      NaN       NaN       NaN       NaN  \n",
      "6                      NaN       NaN       NaN       NaN  \n",
      "7                      4.0  40.34799  36.87143  34.02723  \n",
      "8                      4.0  38.24962  40.34799  32.25537  \n",
      "9                      4.0  29.23187  38.24962  33.48902  \n",
      "\n",
      "[10 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/alitahseen/Desktop/FYP-2024/Machine_learning/Datafiles/ML_Dataset.csv'\n",
    "original_dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Datetime' to datetime and sort by this column\n",
    "original_dataset['Datetime'] = pd.to_datetime(original_dataset['Datetime'])\n",
    "original_dataset.sort_values('Datetime', inplace=True)\n",
    "\n",
    "# Extracting time-related features from 'Datetime'\n",
    "original_dataset['hour'] = original_dataset['Datetime'].dt.hour\n",
    "original_dataset['day_of_week'] = original_dataset['Datetime'].dt.dayofweek\n",
    "\n",
    "# Make a copy of the dataset for LEAR\n",
    "df1 = original_dataset.copy()\n",
    "\n",
    "# Function to create lagged features\n",
    "def create_lagged_features(df, target, lag_days):\n",
    "    for lag in lag_days:\n",
    "        df[f'{target}_lag_{lag}'] = df[target].shift(lag)\n",
    "    return df.dropna()\n",
    "\n",
    "# Creating lagged features for LEAR\n",
    "lag_days = [1, 2, 7]\n",
    "df1 = create_lagged_features(df1, 'MW', lag_days)\n",
    "\n",
    "# Compare rows from the original and transformed datasets\n",
    "# Align and compare the original and transformed datasets based on 'Datetime'\n",
    "comparison = pd.merge(original_dataset, df1, on='Datetime', how='left', suffixes=('_orig', '_transformed'))\n",
    "\n",
    "# Print the comparison for the first 10 rows\n",
    "print(comparison.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAR Train Dataset:\n",
      "     Average_Temp  hour  day_of_week        MW  MW_lag_1  MW_lag_2  MW_lag_7\n",
      "7          3.684     7            4  38.24962  40.34799  36.87143  34.02723\n",
      "8          3.834     8            4  29.23187  38.24962  40.34799  32.25537\n",
      "9          4.966     9            4  22.11242  29.23187  38.24962  33.48902\n",
      "10         6.308    10            4  16.62852  22.11242  29.23187  32.79130\n",
      "11         7.726    11            4  14.51898  16.62852  22.11242  34.10638\n",
      "LEAR Validation Dataset:\n",
      "        Average_Temp  hour  day_of_week        MW  MW_lag_1  MW_lag_2  MW_lag_7\n",
      "18398        10.816    14            0  29.71000  29.51000  33.01000  68.42792\n",
      "18399        10.982    15            0  34.73755  29.71000  29.51000  49.85000\n",
      "18400        10.658    16            0  56.15052  34.73755  29.71000  46.33000\n",
      "18401        10.172    17            0  81.13960  56.15052  34.73755  41.29164\n",
      "18402         8.580    18            0  82.17708  81.13960  56.15052  38.76815\n",
      "LEAR Test Dataset:\n",
      "        Average_Temp  hour  day_of_week         MW   MW_lag_1   MW_lag_2  \\\n",
      "22338        26.994    18            3  160.05493  110.78385   92.22970   \n",
      "22339        24.028    19            3  119.66468  160.05493  110.78385   \n",
      "22340        22.318    20            3   92.47507  119.66468  160.05493   \n",
      "22341        21.830    21            3   75.94872   92.47507  119.66468   \n",
      "22342        21.018    22            3   68.47739   75.94872   92.47507   \n",
      "\n",
      "       MW_lag_7  \n",
      "22338  58.03292  \n",
      "22339  63.11208  \n",
      "22340  68.48224  \n",
      "22341  76.97000  \n",
      "22342  79.04534  \n"
     ]
    }
   ],
   "source": [
    "# For LEAR datasets\n",
    "print(\"LEAR Train Dataset:\\n\", X_train_lear.head())\n",
    "print(\"LEAR Validation Dataset:\\n\", X_val_lear.head())\n",
    "print(\"LEAR Test Dataset:\\n\", X_test_lear.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Train Dataset:\n",
      "    Average_Temp      hour  day_of_week        MW\n",
      "0     -1.213493 -1.660760     0.496702 -0.613609\n",
      "1     -1.300242 -1.516297     0.496702 -0.641057\n",
      "2     -1.347697 -1.371834     0.496702 -0.621946\n",
      "3     -1.318378 -1.227370     0.496702 -0.632755\n",
      "4     -1.382760 -1.082907     0.496702 -0.612383\n",
      "DNN Validation Dataset:\n",
      "    Average_Temp      hour  day_of_week        MW\n",
      "0     -0.675465  0.072797     -1.50184 -0.629367\n",
      "1     -0.503175  0.217260     -1.50184 -0.683586\n",
      "2     -0.362622  0.361723     -1.50184 -0.680488\n",
      "3     -0.337535  0.506186     -1.50184 -0.602605\n",
      "4     -0.386501  0.650649     -1.50184 -0.270893\n",
      "DNN Test Dataset:\n",
      "    Average_Temp      hour  day_of_week        MW\n",
      "0      2.082382  0.939575    -0.002933  1.338711\n",
      "1      1.634127  1.084038    -0.002933  0.713017\n",
      "2      1.375692  1.228501    -0.002933  0.291818\n",
      "3      1.301940  1.372964    -0.002933  0.035805\n",
      "4      1.179221  1.517427    -0.002933 -0.079935\n"
     ]
    }
   ],
   "source": [
    "# For DNN datasets\n",
    "print(\"DNN Train Dataset:\\n\", pd.DataFrame(X_train_dnn_scaled, columns=X_train_dnn.columns).head())\n",
    "print(\"DNN Validation Dataset:\\n\", pd.DataFrame(X_val_dnn_scaled, columns=X_val_dnn.columns).head())\n",
    "print(\"DNN Test Dataset:\\n\", pd.DataFrame(X_test_dnn_scaled, columns=X_test_dnn.columns).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAR Model RMSE: 0.01604265112202082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lear_model = Lasso(alpha=0.1)  # Adjust the alpha parameter as needed\n",
    "\n",
    "# Train the model on the training set\n",
    "lear_model.fit(X_train_lear, y_train_lear)\n",
    "\n",
    "# Make predictions on the test set\n",
    "lear_predictions = lear_model.predict(X_test_lear)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE) for the LEAR model\n",
    "lear_rmse = mean_squared_error(y_test_lear, lear_predictions, squared=False)\n",
    "print(f\"LEAR Model RMSE: {lear_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "575/575 [==============================] - 2s 2ms/step - loss: 1350.8337 - val_loss: 25.6891\n",
      "Epoch 2/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 2.1450 - val_loss: 4.4131\n",
      "Epoch 3/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3257 - val_loss: 2.6791\n",
      "Epoch 4/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1898 - val_loss: 1.9611\n",
      "Epoch 5/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1292 - val_loss: 1.7252\n",
      "Epoch 6/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1035 - val_loss: 1.5415\n",
      "Epoch 7/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0884 - val_loss: 1.4811\n",
      "Epoch 8/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1492 - val_loss: 1.4343\n",
      "Epoch 9/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1105 - val_loss: 1.2624\n",
      "Epoch 10/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2682 - val_loss: 1.2501\n",
      "Epoch 11/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2371 - val_loss: 1.7390\n",
      "Epoch 12/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0948 - val_loss: 1.3200\n",
      "Epoch 13/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 1.1279 - val_loss: 1.4091\n",
      "Epoch 14/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0314 - val_loss: 1.3410\n",
      "Epoch 15/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0250 - val_loss: 1.2958\n",
      "Epoch 16/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0865 - val_loss: 1.2562\n",
      "Epoch 17/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3076 - val_loss: 1.3361\n",
      "Epoch 18/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2183 - val_loss: 1.3311\n",
      "Epoch 19/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2953 - val_loss: 1.8800\n",
      "Epoch 20/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0598 - val_loss: 1.3099\n",
      "Epoch 21/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2764 - val_loss: 1.3179\n",
      "Epoch 22/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1506 - val_loss: 1.3644\n",
      "Epoch 23/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3785 - val_loss: 1.3594\n",
      "Epoch 24/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0245 - val_loss: 1.2898\n",
      "Epoch 25/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2182 - val_loss: 1.3682\n",
      "Epoch 26/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0700 - val_loss: 1.2746\n",
      "Epoch 27/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2852 - val_loss: 1.4276\n",
      "Epoch 28/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0767 - val_loss: 1.2878\n",
      "Epoch 29/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3612 - val_loss: 1.3287\n",
      "Epoch 30/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0626 - val_loss: 1.2871\n",
      "Epoch 31/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3148 - val_loss: 1.9374\n",
      "Epoch 32/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2227 - val_loss: 1.4648\n",
      "Epoch 33/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.7623 - val_loss: 1.5121\n",
      "Epoch 34/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1076 - val_loss: 1.3709\n",
      "Epoch 35/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0047 - val_loss: 1.2539\n",
      "Epoch 36/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0534 - val_loss: 1.4010\n",
      "Epoch 37/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1505 - val_loss: 1.5070\n",
      "Epoch 38/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3431 - val_loss: 1.4003\n",
      "Epoch 39/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0406 - val_loss: 1.3104\n",
      "Epoch 40/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0165 - val_loss: 1.2243\n",
      "Epoch 41/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 2.2680 - val_loss: 1.5115\n",
      "Epoch 42/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0088 - val_loss: 1.3557\n",
      "Epoch 43/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0043 - val_loss: 1.3016\n",
      "Epoch 44/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0038 - val_loss: 1.2535\n",
      "Epoch 45/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0047 - val_loss: 1.1854\n",
      "Epoch 46/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0189 - val_loss: 1.1842\n",
      "Epoch 47/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0219 - val_loss: 1.1370\n",
      "Epoch 48/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.4118 - val_loss: 2.2768\n",
      "Epoch 49/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0702 - val_loss: 1.2309\n",
      "Epoch 50/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 1.1117\n",
      "Epoch 51/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.5431 - val_loss: 1.8003\n",
      "Epoch 52/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0279 - val_loss: 1.1787\n",
      "Epoch 53/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1061 - val_loss: 1.1900\n",
      "Epoch 54/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0013 - val_loss: 1.1073\n",
      "Epoch 55/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3809 - val_loss: 1.2872\n",
      "Epoch 56/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0024 - val_loss: 1.1384\n",
      "Epoch 57/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0106 - val_loss: 1.1184\n",
      "Epoch 58/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.4444 - val_loss: 1.6683\n",
      "Epoch 59/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0041 - val_loss: 1.4688\n",
      "Epoch 60/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.2389 - val_loss: 1.4212\n",
      "Epoch 61/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 1.2802\n",
      "Epoch 62/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0708 - val_loss: 1.2235\n",
      "Epoch 63/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.4062 - val_loss: 1.4090\n",
      "Epoch 64/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 8.4617e-04 - val_loss: 1.2447\n",
      "Epoch 65/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 1.1658\n",
      "Epoch 66/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0089 - val_loss: 1.1496\n",
      "Epoch 67/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1524 - val_loss: 1.3078\n",
      "Epoch 68/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.6198 - val_loss: 1.2981\n",
      "Epoch 69/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 1.2449\n",
      "Epoch 70/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0209 - val_loss: 1.2419\n",
      "Epoch 71/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.3518 - val_loss: 1.2076\n",
      "Epoch 72/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 1.1569\n",
      "Epoch 73/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0017 - val_loss: 1.1903\n",
      "Epoch 74/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.6487 - val_loss: 1.1669\n",
      "Epoch 75/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 3.3396e-04 - val_loss: 1.1067\n",
      "Epoch 76/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 5.2614e-04 - val_loss: 1.0327\n",
      "Epoch 77/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0141 - val_loss: 1.1715\n",
      "Epoch 78/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.4131 - val_loss: 3.4830\n",
      "Epoch 79/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0529 - val_loss: 1.1240\n",
      "Epoch 80/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0024 - val_loss: 1.0569\n",
      "Epoch 81/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0043 - val_loss: 0.9916\n",
      "Epoch 82/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1004 - val_loss: 1.1030\n",
      "Epoch 83/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.5463 - val_loss: 3.0086\n",
      "Epoch 84/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0254 - val_loss: 1.1470\n",
      "Epoch 85/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 5.2311e-04 - val_loss: 1.0910\n",
      "Epoch 86/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 1.1058\n",
      "Epoch 87/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0509 - val_loss: 1.0580\n",
      "Epoch 88/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.1395 - val_loss: 1.1635\n",
      "Epoch 89/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0461 - val_loss: 1.0958\n",
      "Epoch 90/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0512 - val_loss: 1.0798\n",
      "Epoch 91/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0310 - val_loss: 1.0777\n",
      "Epoch 92/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0614 - val_loss: 1.0611\n",
      "Epoch 93/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.5998 - val_loss: 1.0869\n",
      "Epoch 94/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 6.6310e-04 - val_loss: 1.1205\n",
      "Epoch 95/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 9.5191e-04 - val_loss: 1.1322\n",
      "Epoch 96/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 0.0047 - val_loss: 1.0686\n",
      "Epoch 97/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 1.0585 - val_loss: 1.2385\n",
      "Epoch 98/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 4.0402e-04 - val_loss: 1.1844\n",
      "Epoch 99/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 4.2380e-04 - val_loss: 1.0535\n",
      "Epoch 100/100\n",
      "575/575 [==============================] - 1s 2ms/step - loss: 2.9919e-04 - val_loss: 1.0392\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.0054e-04\n",
      "DNN Model Loss: 0.00010053654841613024\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the DNN model architecture\n",
    "dnn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_dnn_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the DNN model\n",
    "dnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the DNN model\n",
    "dnn_history = dnn_model.fit(\n",
    "    X_train_dnn_scaled, \n",
    "    y_train_dnn, \n",
    "    validation_data=(X_val_dnn_scaled, y_val_dnn), \n",
    "    epochs=100, \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the DNN model on the test set\n",
    "dnn_loss = dnn_model.evaluate(X_test_dnn_scaled, y_test_dnn)\n",
    "print(f\"DNN Model Loss: {dnn_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAR Model MAE: 0.005854369392270171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate MAE for LEAR model\n",
    "lear_mae = mean_absolute_error(y_test_lear, lear_predictions)\n",
    "print(f\"LEAR Model MAE: {lear_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 1ms/step\n",
      "LEAR Model MAE: 0.005118332699650697, RMSE: 0.01604265112202082, MAPE: 0.007529578709421786%, sMAPE: 0.007529439343802482%\n",
      "DNN Model MAE: 0.008061230883779729, RMSE: 0.010026841746419856, MAPE: 0.016359422832727632%, sMAPE: 0.01636049810801308%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate MAPE and sMAPE with a safeguard for zero values\n",
    "def mape(y_true, y_pred, epsilon=1e-8):\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def smape(y_true, y_pred, epsilon=1e-8):\n",
    "    masked_y_true = y_true + epsilon\n",
    "    return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(masked_y_true) + np.abs(y_pred))) * 100\n",
    "\n",
    "# Calculate metrics for LEAR model\n",
    "lear_mae = mean_absolute_error(y_test_lear, lear_predictions)\n",
    "lear_rmse = np.sqrt(mean_squared_error(y_test_lear, lear_predictions))\n",
    "lear_mape = mape(y_test_lear, lear_predictions)\n",
    "lear_smape = smape(y_test_lear, lear_predictions)\n",
    "\n",
    "# Make predictions with the DNN model\n",
    "dnn_predictions = dnn_model.predict(X_test_dnn_scaled).flatten()\n",
    "\n",
    "# Calculate metrics for DNN model\n",
    "dnn_mae = mean_absolute_error(y_test_dnn, dnn_predictions)\n",
    "dnn_rmse = np.sqrt(mean_squared_error(y_test_dnn, dnn_predictions))\n",
    "dnn_mape = mape(y_test_dnn, dnn_predictions)\n",
    "dnn_smape = smape(y_test_dnn, dnn_predictions)\n",
    "\n",
    "# Print metrics for both models\n",
    "print(f\"LEAR Model MAE: {lear_mae}, RMSE: {lear_rmse}, MAPE: {lear_mape}%, sMAPE: {lear_smape}%\")\n",
    "print(f\"DNN Model MAE: {dnn_mae}, RMSE: {dnn_rmse}, MAPE: {dnn_mape}%, sMAPE: {dnn_smape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAR Model Predictions vs Actual:\n",
      "                     Date  Actual LMP  Predicted LMP by LEAR\n",
      "22338 2023-07-20 18:00:00   160.05493             160.025553\n",
      "22339 2023-07-20 19:00:00   119.66468             119.734409\n",
      "22340 2023-07-20 20:00:00    92.47507              92.467893\n",
      "22341 2023-07-20 21:00:00    75.94872              75.942562\n",
      "22342 2023-07-20 22:00:00    68.47739              68.471584\n",
      "\n",
      "DNN Model Predictions vs Actual:\n",
      "                     Date  Actual LMP  Predicted LMP by DNN\n",
      "22338 2023-07-20 18:00:00   160.05493            160.047012\n",
      "22339 2023-07-20 19:00:00   119.66468            119.658501\n",
      "22340 2023-07-20 20:00:00    92.47507             92.469872\n",
      "22341 2023-07-20 21:00:00    75.94872             75.945580\n",
      "22342 2023-07-20 22:00:00    68.47739             68.478813\n"
     ]
    }
   ],
   "source": [
    "# Combine the date, actual values, and predictions into a DataFrame\n",
    "lear_comparison_df = pd.DataFrame({\n",
    "    'Date': test_dates,\n",
    "    'Actual LMP': y_test_lear,\n",
    "    'Predicted LMP by LEAR': lear_predictions\n",
    "})\n",
    "\n",
    "dnn_comparison_df = pd.DataFrame({\n",
    "    'Date': test_dates,\n",
    "    'Actual LMP': y_test_dnn,\n",
    "    'Predicted LMP by DNN': dnn_predictions\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"LEAR Model Predictions vs Actual:\")\n",
    "print(lear_comparison_df.head())\n",
    "\n",
    "print(\"\\nDNN Model Predictions vs Actual:\")\n",
    "print(dnn_comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE scores for LEAR: [0.02279702 0.01045968 0.02358765 0.01057559 0.01450298]\n",
      "Mean RMSE: 0.016384583576223046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize the Lasso model with the same alpha as before\n",
    "lear_model_cv = Lasso(alpha=0.1)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(lear_model_cv, df1[feature_columns_lear], df1['MW'], cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive values and take the square root to get RMSE\n",
    "rmse_scores = np.sqrt(-cross_val_scores)\n",
    "print(f\"Cross-validated RMSE scores for LEAR: {rmse_scores}\")\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Actual LMP  Predicted LMP on Shuffled Data by LEAR\n",
      "22338   160.05493                               62.387002\n",
      "22339   119.66468                               59.284289\n",
      "22340    92.47507                               48.256013\n",
      "22341    75.94872                               69.080195\n",
      "22342    68.47739                               48.901083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle the test features\n",
    "X_test_lear_shuffled = shuffle(X_test_lear, random_state=42)\n",
    "\n",
    "# Make predictions on the shuffled test set\n",
    "lear_predictions_shuffled = lear_model.predict(X_test_lear_shuffled)\n",
    "\n",
    "# Compare with actual values\n",
    "shuffled_comparison_df = pd.DataFrame({\n",
    "    'Actual LMP': y_test_lear,\n",
    "    'Predicted LMP on Shuffled Data by LEAR': lear_predictions_shuffled\n",
    "})\n",
    "print(shuffled_comparison_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
